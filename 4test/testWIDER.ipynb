{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path\n",
    "import sys\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "class WiderFaceDetection(data.Dataset):\n",
    "    def __init__(self, txt_path, preproc=None):\n",
    "        self.preproc = preproc\n",
    "        self.imgs_path = []\n",
    "        self.words = []\n",
    "        f = open(txt_path,'r')\n",
    "        lines = f.readlines()\n",
    "        isFirst = True\n",
    "        labels = []\n",
    "        for line in lines:\n",
    "            line = line.rstrip()\n",
    "            if line.startswith('#'):\n",
    "                if isFirst is True:\n",
    "                    isFirst = False\n",
    "                else:\n",
    "                    labels_copy = labels.copy()\n",
    "                    self.words.append(labels_copy)\n",
    "                    labels.clear()\n",
    "                path = line[2:]\n",
    "                path = txt_path.replace('label.txt','images/') + path\n",
    "                self.imgs_path.append(path)\n",
    "            else:\n",
    "                line = line.split(' ')\n",
    "                label = [float(x) for x in line]\n",
    "                labels.append(label)\n",
    "\n",
    "        self.words.append(labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs_path)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = cv2.imread(self.imgs_path[index])\n",
    "        height, width, _ = img.shape\n",
    "\n",
    "        labels = self.words[index]\n",
    "        annotations = np.zeros((0, 15))\n",
    "        if len(labels) == 0:\n",
    "            return annotations\n",
    "        for idx, label in enumerate(labels):\n",
    "            annotation = np.zeros((1, 15))\n",
    "            # bbox\n",
    "            annotation[0, 0] = label[0]  # x1\n",
    "            annotation[0, 1] = label[1]  # y1\n",
    "            annotation[0, 2] = label[0] + label[2]  # x2\n",
    "            annotation[0, 3] = label[1] + label[3]  # y2\n",
    "\n",
    "            # landmarks\n",
    "            annotation[0, 4] = label[4]    # l0_x\n",
    "            annotation[0, 5] = label[5]    # l0_y\n",
    "            annotation[0, 6] = label[7]    # l1_x\n",
    "            annotation[0, 7] = label[8]    # l1_y\n",
    "            annotation[0, 8] = label[10]   # l2_x\n",
    "            annotation[0, 9] = label[11]   # l2_y\n",
    "            annotation[0, 10] = label[13]  # l3_x\n",
    "            annotation[0, 11] = label[14]  # l3_y\n",
    "            annotation[0, 12] = label[16]  # l4_x\n",
    "            annotation[0, 13] = label[17]  # l4_y\n",
    "            if (annotation[0, 4]<0):\n",
    "                annotation[0, 14] = -1\n",
    "            else:\n",
    "                annotation[0, 14] = 1\n",
    "\n",
    "            annotations = np.append(annotations, annotation, axis=0)\n",
    "        target = np.array(annotations)\n",
    "        if self.preproc is not None:\n",
    "            img, target = self.preproc(img, target)\n",
    "        # return img, target [bbox, loc mat mui mieng]\n",
    "        return torch.from_numpy(img), target\n",
    "\n",
    "def detection_collate(batch):\n",
    "    \"\"\"Custom collate fn for dealing with batches of images that have a different\n",
    "    number of associated object annotations (bounding boxes).\n",
    "\n",
    "    Arguments:\n",
    "        batch: (tuple) A tuple of tensor images and lists of annotations\n",
    "\n",
    "    Return:\n",
    "        A tuple containing:\n",
    "            1) (tensor) batch of images stacked on their 0 dim\n",
    "            2) (list of tensors) annotations for a given image are stacked on 0 dim\n",
    "    \"\"\"\n",
    "    targets = []\n",
    "    imgs = []\n",
    "    for _, sample in enumerate(batch):\n",
    "        for _, tup in enumerate(sample):\n",
    "            if torch.is_tensor(tup):\n",
    "                imgs.append(tup)\n",
    "            elif isinstance(tup, type(np.empty(0))):\n",
    "                annos = torch.from_numpy(tup).float()\n",
    "                targets.append(annos)\n",
    "\n",
    "    return (torch.stack(imgs, 0), targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = WiderFaceDetection('e:/MY/widerface/train/label.txt',None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[255, 251, 237],\n",
      "         [255, 251, 237],\n",
      "         [255, 251, 237],\n",
      "         ...,\n",
      "         [249, 250, 248],\n",
      "         [250, 255, 255],\n",
      "         [233, 243, 243]],\n",
      "\n",
      "        [[106,  72,  59],\n",
      "         [106,  72,  59],\n",
      "         [106,  72,  59],\n",
      "         ...,\n",
      "         [173, 169, 168],\n",
      "         [245, 249, 250],\n",
      "         [247, 255, 255]],\n",
      "\n",
      "        [[ 94,  50,  37],\n",
      "         [ 94,  50,  37],\n",
      "         [ 94,  50,  37],\n",
      "         ...,\n",
      "         [172, 162, 162],\n",
      "         [255, 254, 255],\n",
      "         [253, 255, 255]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 35,  73,  67],\n",
      "         [ 34,  72,  66],\n",
      "         [ 32,  70,  64],\n",
      "         ...,\n",
      "         [146, 156, 156],\n",
      "         [247, 255, 255],\n",
      "         [248, 253, 254]],\n",
      "\n",
      "        [[ 42,  80,  74],\n",
      "         [ 40,  78,  72],\n",
      "         [ 36,  74,  68],\n",
      "         ...,\n",
      "         [146, 156, 156],\n",
      "         [247, 255, 255],\n",
      "         [248, 253, 254]],\n",
      "\n",
      "        [[ 40,  78,  72],\n",
      "         [ 38,  76,  70],\n",
      "         [ 35,  73,  67],\n",
      "         ...,\n",
      "         [146, 156, 156],\n",
      "         [248, 255, 255],\n",
      "         [249, 254, 255]]], dtype=torch.uint8), array([[449.   , 330.   , 571.   , 479.   , 488.906, 373.643, 542.089,\n",
      "        376.442, 515.031, 412.83 , 485.174, 425.893, 538.357, 431.491,\n",
      "          1.   ]]))\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
